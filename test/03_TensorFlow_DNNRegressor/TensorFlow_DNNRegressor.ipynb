{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "731ffa18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:31:27.800884Z",
     "start_time": "2023-04-24T07:31:27.780995Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#基于Python TensorFlow Estimator DNNRegressor的深度学习回归\n",
    "#1 导入相关的库和包\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import tensorflow as tf\n",
    "\n",
    "#基于TensorFlow的代码往往会输出较多的日志信息，从而使得我们对代码执行情况的了解受到一定影响。代码输出的日志信息有四种，\n",
    "#依据严重程度由低到高排序：INFO（通知）<WARNING（警告）<ERROR（错误）<FATAL（致命的）；\n",
    "#通过os.environ对TensorFlow的输出日志信息加以约束，“3”代表只输出FATAL信息。这句代码需放在 import tensorflow的前面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "981584ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:41:17.372596Z",
     "start_time": "2023-04-24T07:41:17.359640Z"
    },
    "code_folding": [
     2,
     4,
     20,
     27,
     31,
     52
    ]
   },
   "outputs": [],
   "source": [
    "#函数functions定义区\n",
    "\n",
    "#DeleteOldModel函数，删除上一次运行所保存的模型\n",
    "#需要注意，以下代码仅删除指定路径下的文件，文件夹不删除。大家如果需要将文件夹也同时删除，修改以上代码函数中的后面几句即可。\n",
    "def DeleteOldModel(ModelPath):\n",
    "    AllFileName=os.listdir(ModelPath) # 获取ModelPath路径下全部文件与文件夹\n",
    "    for i in AllFileName:\n",
    "        NewPath=os.path.join(ModelPath,i) # 分别将所获取的文件或文件夹名称与ModelPath路径组合\n",
    "        if os.path.isdir(NewPath): # 若组合后的新路径是一个文件夹\n",
    "            DeleteOldModel(NewPath) # 递归调用DeleteOldModel函数\n",
    "        else:\n",
    "            os.remove(NewPath) # 若不是一个新的文件夹，而是一个文件，那么就删除\n",
    "\n",
    "# LoadData函数，加载全部数据          \n",
    "def LoadData(DataPath):\n",
    "    MyData = pd.read_csv(DataPath, names=['Petrol_tax','Average_income','Paved_Highways','Population_Driver_licence',\n",
    "                                      'Petrol_Consumption'],header=0) #加载DataPath路径所指定的数据，names中的内容为各列的名称\n",
    "    return MyData\n",
    "\n",
    "# InputFun函数，训练数据与验证数据所用的Input函数\n",
    "def InputFun(Features,Labels,Training,BatchSize):\n",
    "    Datasets=tf.data.Dataset.from_tensor_slices((dict(Features),Labels)) # 对数据加以加载\n",
    "    if Training:\n",
    "        Datasets=Datasets.shuffle(1000).repeat() # 对于训练数据，需要打乱（shuffle）、重复（repeat）\n",
    "    return Datasets.batch(BatchSize) # 将经过上述处理后的数据以每次BatchSize个输出\n",
    "\n",
    "# InputFunPredict函数，测试数据所用的Input函数\n",
    "def InputFunPredict(Features,BatchSize):\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(Features)).batch(BatchSize) # 对数据加以加载,以每次BatchSize个输出\n",
    "\n",
    "# AccuracyVerification函数，进行精度验证指标的计算与绘图\n",
    "def AccuracyVerification(PredictLabels,TestLabels):\n",
    "    value=0\n",
    "    PredictValuesList=[]\n",
    "    for k in PredictLabels:\n",
    "        value=k.get('predictions')[0]\n",
    "        PredictValuesList.append(value)\n",
    "    TestLabels=TestLabels.values.tolist()\n",
    "    TestYList=sum(TestLabels,[])\n",
    "    # 以上为获取测试数据的因变量与模型预测所得的因变量\n",
    "    Pearsonr=stats.pearsonr(TestYList,PredictValuesList) # 计算皮尔逊相关系数\n",
    "    R2=metrics.r2_score(TestYList,PredictValuesList) # 计算R方\n",
    "    RMSE=metrics.mean_squared_error(TestYList,PredictValuesList)**0.5 # 计算RMSE\n",
    "    plt.cla()\n",
    "    plt.plot(TestYList,PredictValuesList,'r*')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    # 以上为绘制拟合图像\n",
    "    print('Pearson correlation coefficient is {0}, and RMSE is {1}.'.format(Pearsonr[0],RMSE))\n",
    "    return (Pearsonr[0],R2,RMSE,PredictValuesList)\n",
    "\n",
    "# WriteAccuracy函数，将模型所涉及的参数与最终精度结果保存\n",
    "def WriteAccuracy(*WriteVar):\n",
    "    ExcelData=openpyxl.load_workbook(WriteVar[0])\n",
    "    SheetName=ExcelData.get_sheet_names() # 获取全部Sheet\n",
    "    WriteSheet=ExcelData.get_sheet_by_name(SheetName[0]) # 获取指定Sheet\n",
    "    WriteSheet=ExcelData.active # 激活指定Sheet\n",
    "    MaxRowNum=WriteSheet.max_row # 获取指定Sheet对应第一个空行\n",
    "    for i in range(len(WriteVar)-1):\n",
    "        exec(\"WriteSheet.cell(MaxRowNum+1,i+1).value=WriteVar[i+1]\") # 用exec执行语句，写入信息\n",
    "    ExcelData.save(WriteVar[0]) # 保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ccd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码由此开始执行\n",
    "#2 参数配置\n",
    "#深度学习代码一大特点即为具有较多的参数需要我们手动定义。为避免调参时上下翻找，我们可以将主要的参数集中在一起，方便我们后期调整。\n",
    "#将各类变量放在一个位置集中定义，十分有利于机器学习等变量较多的代码\n",
    "\n",
    "MyModelPath=\"02_DNNModle/\" # 确定每一次训练所得模型保存的位置\n",
    "MyDataPath=\"00_Data/AllDataAll.csv\" # 确定输入数据的位置\n",
    "MyResultSavePath=\"03_OtherResult/EvalResult54.xlsx\" # 确定模型精度结果（RMSE等）与模型参数保存的位置\n",
    "\n",
    "TestSize=0.2 # 确定数据中测试集所占比例\n",
    "RandomSeed=np.random.randint(low=24,high=25) # 确定划分训练集与测试集的随机数种子\n",
    "OptMethod='Adam' # 确定模型所用的优化方法\n",
    "LearningRate=0.01 # 确定学习率\n",
    "DecayStep=200 # 确定学习率下降的步数\n",
    "DecayRate=0.96 # 确定学习率下降比率\n",
    "HiddenLayer=[64,128] # 确定隐藏层数量与每一层对应的神经元数量\n",
    "ActFun='tf.nn.relu' # 确定激活函数\n",
    "Dropout=0.3 # 确定Dropout的值\n",
    "LossReduction='tf.compat.v1.ReductionV2.SUM_OVER_BATCH_SIZE' # 指定每个批次训练误差的减小方法\n",
    "BatchNorm='False' # 确定是否使用Batch Normalizing\n",
    "TrainBatchSize=110 # 确定训练数据一个Batch的大小\n",
    "TrainStep=3000 # 确定训练数据的Step数量\n",
    "EvalBatchSize=1 # 确定验证数据一个Batch的大小\n",
    "PredictBatchSize=1 # 确定预测数据（即测试集）一个Batch的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46bf3408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:43:40.637940Z",
     "start_time": "2023-04-24T07:43:40.628971Z"
    }
   },
   "outputs": [],
   "source": [
    "#3 原有模型删除\n",
    "#DNNRegressor每执行一次，便会在指定路径中保存当前运行的模型。\n",
    "#为保证下一次模型保存时不受上一次模型运行结果干扰，我们可以将模型文件夹内的全部文件删除。\n",
    "\n",
    "# 调用DeleteOldModel函数，删除上一次运行所保存的模型\n",
    "DeleteOldModel(MyModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11f6bc35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:44:59.523480Z",
     "start_time": "2023-04-24T07:44:59.509927Z"
    }
   },
   "outputs": [],
   "source": [
    "#4 数据导入与数据划分\n",
    "#初始数据集划分为训练集与测试集，数据训练集与测试集的划分在机器学习、深度学习中是不可或缺的作用\n",
    "#数据保存在csv文件中，因此可以用pd.read_csv直接读取。\n",
    "#其中，数据的每一列是一个特征，每一行是全部特征与因变量（就是下面的Yield）组合成的样本。\n",
    "    \n",
    "#初始数据处理\n",
    "AllXY = LoadData(MyDataPath) # 调用LoadData函数，获取数据\n",
    "#print(AllXY)\n",
    "Label={\"Petrol_Consumption\":AllXY.pop(\"Petrol_Consumption\")} # 将因变量从全部数据中提取出\n",
    "AllX,AllY=AllXY,(pd.DataFrame(Label)) # 将自变量与因变量分离\n",
    "#print(AllY)\n",
    "\n",
    "#划分数据训练集与测试集\n",
    "TrainX, TestX, TrainY,TestY= train_test_split(AllX,\n",
    "                                              AllY,\n",
    "                                              test_size=TestSize,  # 指定数据中测试集所占比例\n",
    "                                              random_state=RandomSeed) # 指定划分训练集与测试集的随机数种子\n",
    "#print(TestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5c31797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:45:32.684911Z",
     "start_time": "2023-04-24T07:45:32.672026Z"
    }
   },
   "outputs": [],
   "source": [
    "#5 Feature Columns定义\n",
    "#Feature Columns就是一个桥梁，联系你的初始数据与模型；\n",
    "#其好比一个名单，模型拿着这个名单到你的数据（即#4部分你导入的数据）中按列的名称一一搜索，\n",
    "#若初始数据中的某列名称在Feature Columns里，那么模型就会把初始数据中这一列的数据全部拿到自己这里，进行训练。\n",
    "#因为我们是希望导入数据的全部特征，那么可以直接在全部数据的自变量中循环，将全部特征的名称导入Feature Columns。\n",
    "#在这里需要注意的是，只有连续数值变量才可以用tf.feature_column.numeric_column处理；若是类别变量可以参考：\n",
    "#https://blog.csdn.net/zhebushibiaoshifu/article/details/115335441。\n",
    "\n",
    "# estimator接口中的模型需要用“Feature columns”对象作为输入数据，只有这样模型才知道读取哪些数据\n",
    "FeatureColumn=[] # 定义一个新的“Feature columns”对象\n",
    "for key in AllX.keys():\n",
    "    FeatureColumn.append(tf.feature_column.numeric_column(key=key)) # 将全部因变量数据（需要均为连续变量）导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74767a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:13:39.374372Z",
     "start_time": "2023-04-24T07:13:39.351434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '02_DNNModle/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#6 模型优化方法构建与模型结构构建\n",
    "#模型优化方法即模型中的optimizer，其可以在模型结构构建时输入；\n",
    "#但有时优化方法较为复杂（例如引入了学习率下降），那么在构建模型时配置优化方法的话就会有些不方便。因此我们首先构建模型优化方法。\n",
    "\n",
    "# 定义模型优化方法\n",
    "# Optimizer=OptMethod # 优化方法选用OptMethod所指定的方法  第一个\n",
    "# Optimizer=OptMethod # 优化方法选用OptMethod所指定的方法\n",
    "Optimizer=lambda:tf.keras.optimizers.Adam(\n",
    "    learning_rate=tf.compat.v1.train.exponential_decay(learning_rate=LearningRate, # 初始学习率\n",
    "                                                       global_step=tf.compat.v1.train.get_global_step(),\n",
    "                                                       # 全局步数，用以计算已经衰减后的学习率\n",
    "                                                       # get_global_step()函数自动获取当前的已经执行的步数\n",
    "                                                       decay_steps=DecayStep, # 学习率下降完成的指定步数\n",
    "                                                       decay_rate=DecayRate # 衰减率\n",
    "                                                       ) # 选用基于学习率指数下降的Adam方法，此举有助于降低过拟合风险\n",
    "                                                         # 这一函数返回每次对应的学习率\n",
    "    )\n",
    "\n",
    "#以上代码中有两个Optimizer=，第一个是直接输入优化方法的名称即可，名称包括：'Adagrad', 'Adam', 'Ftrl', 'RMSProp', SGD'；默认为Adagrad。\n",
    "#第二个是在选择了优化方法的基础上，配置其他信息。例如第二个，其代表着学习率指数下降的Adam优化方法。\n",
    "#其中，tf.compat.v1.train.exponential_decay可视作一个计算每次训练学习率的函数，他返回的是每一次对应的学习率。\n",
    "#可能这么说不太好理解，看这个公式：其返回值为 \n",
    "#   learning_rate *decay_rate^(global_step / decay_steps)，是不是就明白啦。\n",
    "#我们选择第二个优化方法，因此把第一个注释掉。\n",
    "\n",
    "#随后，我们定义模型的结构\n",
    "# 基于DNNRegressor构建深度学习模型\n",
    "DNNModel=tf.estimator.DNNRegressor(feature_columns=FeatureColumn, # 指定模型所用的“Feature columns”对象\n",
    "                                   hidden_units=HiddenLayer, # 指定隐藏层数量与每一层对应的神经元数量\n",
    "                                   optimizer=Optimizer, # 指定模型所用的优化方法\n",
    "                                   activation_fn=eval(ActFun), # 指定激活函数\n",
    "                                   dropout=Dropout,  # 指定Dropout的值\n",
    "                                   label_dimension=1, # 输出数据的维度，即因变量的个数\n",
    "                                   model_dir=MyModelPath, # 指定每一次训练所得模型保存的位置\n",
    "                                   #loss_reduction=eval(LossReduction), # 指定每个批次训练误差的减小方法\n",
    "                                   batch_norm=eval(BatchNorm) # 指定是否使用Batch Normalizing\n",
    "                                  )\n",
    "\n",
    "#其中，我把loss_reduction注释掉，是因为可能由于TensorFlow版本的问题，其总是报错，所以就用默认的值就好；\n",
    "#而最后一个batch_norm，决定了是否进行Batch Normalizing。\n",
    "#Batch Normalizing可以保持深度神经网络在每一层保持相同分布，从而加快网络收敛与增强网络稳固性。\n",
    "\n",
    "#其它参数可以参考：\n",
    "#https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor；\n",
    "#https://www.tensorflow.org/api_docs/python/tf/nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d950a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T07:26:54.992401Z",
     "start_time": "2023-04-24T07:26:54.872190Z"
    },
    "code_folding": [
     10,
     14,
     28
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please set your optimizer as an instance of `tf.keras.optimizers.legacy.Optimizer`, e.g., `tf.keras.optimizers.legacy.Adam`.Received optimizer type: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7216\\2663247152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# 基于训练数据训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m DNNModel.train(input_fn=lambda:InputFun(TrainX, TrainY,True,TrainBatchSize),#调用InputFun函数；InputFun函数返回“tf.data.Dataset”对象，这个对象才可以被train函数识别并带入模型；由于InputFun函数每次返回BatchSize大小的数据个数，因此需要多次执行，前面需要加lambda              \n\u001b[1;32m---> 43\u001b[1;33m                steps=TrainStep) # 指定模型训练的步数\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#可以这么理解：在train函数中，只有一个参数input_fn；\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1184\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1213\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[1;32m-> 1215\u001b[1;33m                                            self.config)\n\u001b[0m\u001b[0;32m   1216\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[1;34m(features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1168\u001b[0m           \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m           batch_norm=batch_norm)\n\u001b[0m\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m     super(DNNRegressorV2, self).__init__(\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\u001b[0m in \u001b[0;36mdnn_model_fn_v2\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    568\u001b[0m   \u001b[1;31m# relies on global step as step counter.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_optimizer_instance_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda\\envs\\cudaOCRpy37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\optimizers.py\u001b[0m in \u001b[0;36mget_optimizer_instance_v2\u001b[1;34m(opt, learning_rate)\u001b[0m\n\u001b[0;32m    144\u001b[0m       \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_legacy_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m       raise ValueError('Please set your optimizer as an instance of '\n\u001b[0m\u001b[0;32m    147\u001b[0m                        \u001b[1;34m'`tf.keras.optimizers.legacy.Optimizer`, e.g., '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                        \u001b[1;34mf'`tf.keras.optimizers.legacy.{opt.__class__.__name__}`.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please set your optimizer as an instance of `tf.keras.optimizers.legacy.Optimizer`, e.g., `tf.keras.optimizers.legacy.Adam`.Received optimizer type: <class 'keras.optimizers.optimizer_experimental.adam.Adam'>."
     ]
    }
   ],
   "source": [
    "#7 模型训练\n",
    "#训练模型这一部分，我认为反而比模型的构建可能还难理解一些。我们先看代码\n",
    "\n",
    "\n",
    "\n",
    "#那我们首先就看input function——也就是代码中的InputFun函数。其实这个函数的用处很简单，用官网的话说，\n",
    "#其就是用来输入模型支持的数据类型的——只有经过input function处理后，数据才可以被DNNRegressor识别。\n",
    "#听上去这么厉害，它到底是如何操作的呢？\n",
    "\n",
    "#很简单，它只需要将初始的数据转换为特定的格式即可，这个格式是一个元组（tuple），这个元组有两个元素：\n",
    "#一就是features，是一个字典。这个字典的每一个键是每一个特征的名称，\n",
    "#就比如用植物特性对花的种类加以区分，那么花的“叶长”“叶片厚度”等等就是一个个特征的名称，也就是这里的一个个“键”；\n",
    "#而这个字典的值，就是这个特征对应的全部样本的数值组成的数组。\n",
    "#二就是label，是全部样本对应的label，也就是因变量。\n",
    "#不知道大家有没有理解，我们就举一个简单的例子。\n",
    "#假如我们用两个地方的温度与降水预测这两个地方的作物产量：其温度分别为10 ℃、20 ℃，降水分别为15 mm，25 mm，\n",
    "#作物产量分别为100千克每公顷，150千克每公顷——那么tuple由两个部分组成：\n",
    "#tuple=(features,label)\n",
    "#features={'温度':np.array([10,20]),'降水':np.array([15,25])}\n",
    "#label=np.array([100,150])\n",
    "#tuple=(features,label)\n",
    "#print(tuple)\n",
    "\n",
    "#理解了之后，我们继续看InputFun函数。\n",
    "#首先，tf.data.Dataset.from_tensor_slices用来将输入的数据加载并转换为Datase的形式；\n",
    "#随后，如果是训练状态下，那么数据会进行打乱.shuffle(1000)——相当于对数据加以洗牌，防止初始数据具有一定的趋势。\n",
    "#例如如果我们做分类，其中初始数据的前80%都是第一类，后20%都是第二类，\n",
    "#那么如果我们不打乱数据，会使得用前80%数据训练出来的结果都是第一类（即模型只认识第一类），\n",
    "#在后20%进行测试时，所得结果也全都为第一类；所以要打乱。\n",
    "#其中的1000是buffer_size参数，这个数据必须要比你的数据样本个数大。至于.shuffle(1000)这个函数的原理我一直没有搞明白，大家感兴趣的话可以了解一下：\n",
    "#https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle。\n",
    "#.repeat()则是对数据集加以重复，之所以要重复，是因为我们需要对全部数据训练好几轮（即好几个Epoch），因此要对初始数据加以重复。\n",
    "#随后，用.batch()函数输出BatchSize个数据，也就是一批数据；其中BatchSize就是每一批数据的个数。\n",
    "#以上就是InputFun函数。\n",
    "\n",
    "# 基于训练数据训练模型\n",
    "DNNModel.train(input_fn=lambda:InputFun(TrainX, TrainY,True,TrainBatchSize),#调用InputFun函数；InputFun函数返回“tf.data.Dataset”对象，这个对象才可以被train函数识别并带入模型；由于InputFun函数每次返回BatchSize大小的数据个数，因此需要多次执行，前面需要加lambda              \n",
    "               steps=TrainStep) # 指定模型训练的步数\n",
    "\n",
    "#可以这么理解：在train函数中，只有一个参数input_fn；\n",
    "#而这个参数的输入，又是一个新的函数——这个新的函数就是大名鼎鼎的input function了,参考上面。\n",
    "#再看train函数：大家也看出来了，这个InputFun函数是每次输出一批（BatchSize个）数据；\n",
    "#而我们训练的时候，肯定是要一批一批不停输入数据的，因此这就解释了为什么InputFun函数前有一个lambda——因为InputFun函数\n",
    "#要把处理后的数据分多次传给train。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14555c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaOCRpy37",
   "language": "python",
   "name": "cudaocrpy37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
